{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7efba7-48f2-43c5-9355-9e5b98a61e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8cfbf-6573-493c-9613-f0fd639a6577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e067a72-7075-4ced-b450-f38a530a7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell is a v0 of the idea. It behaves well with GPT3.5-turbo so I decided to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efee9e4-9440-4e3f-8ec4-47e57b4b42bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def generate_story_outline(logline):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant with expertise in screenwriting and Truby's story structure. You are working on a short story so make sure the pace is fast.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Outline a short story based on this logline using Truby's story structure: {logline}. Include the hero's goal, the opponent, the conflict, the mentor, the theme, the hero's character change, and a big reveal or twist that will leave the audience speechless.\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    story_outline = response['choices'][0]['message']['content']\n",
    "    return story_outline\n",
    "\n",
    "def generate_chapter_outlines(story_outline):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant with expertise in screenwriting and Truby's story structure.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Based on this short story outline: {story_outline}, generate short chapter outlines for the short story. Answer in JSON format where chapters are in a list inside the chapters attribute. It should look like this: {{\n",
    "   \"title\": \"the title\",\n",
    "   \"author\": \"the author\",\n",
    "   \"genre\": \"the genre\",\n",
    "   \"chapters\": [\n",
    "      {{\n",
    "         \"chapter_number\": 1,\n",
    "         \"chapter_title\": \"The title\",\n",
    "         \"chapter_summary\": \"the summary\"\n",
    "      }}\n",
    "   ]\n",
    "}}\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    chapter_outlines_str = response['choices'][0]['message']['content']\n",
    "    novel_structure = json.loads(chapter_outlines_str)\n",
    "    chapter_outlines = novel_structure[\"chapters\"]\n",
    "    print(chapter_outlines)\n",
    "    return chapter_outlines\n",
    "\n",
    "def refine_chapter_segment(segment, prior_messages):\n",
    "    messages = prior_messages + [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can rewrite text in the style of a New York Times bestseller.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Rewrite this text: {segment}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    refined_segment = response['choices'][0]['message']['content']\n",
    "    return refined_segment\n",
    "\n",
    "def write_chapter(chapter_outline, logline, chapters):\n",
    "    chapter = \"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a chapter for a short story based on this outline: {chapter_outline}. The logline of the story is this: {logline}. This is the full list of the chapters, but remember you are only writing one of them. {chapters} When the chapter is done write END CHAPTER\"}\n",
    "    ]\n",
    "    while True:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=2048,\n",
    "            stop=[\"END CHAPTER\"]\n",
    "        )\n",
    "\n",
    "        content = response['choices'][0]['message']['content']\n",
    "        refined_content = refine_chapter_segment(content, messages[-30:])\n",
    "        chapter += refined_content\n",
    "        #print(f\"OUTLINE: {chapter_outline}\")\n",
    "        print(\"\\n\")\n",
    "        print(refined_content)\n",
    "        \n",
    "        if \"END CHAPTER\" in chapter or len(chapter.split()) > 2000:\n",
    "            print(\"CHAPTER IS FINISHED\")\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": \"Continue the chapter.\"})\n",
    "\n",
    "    return chapter\n",
    "\n",
    "def evaluate_and_refine_chapter(chapter):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can evaluate and refine written work.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Evaluate and refine this chapter: {chapter}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "\n",
    "    refined_chapter = response['choices'][0]['message']['content']\n",
    "    print(refined_chapter)\n",
    "    return refined_chapter\n",
    "\n",
    "\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "logline = \"A teenager gets to meet her older self from the future\"\n",
    "story_outline = generate_story_outline(logline)\n",
    "print(story_outline)\n",
    "chapter_outlines = generate_chapter_outlines(story_outline)\n",
    "\n",
    "output_filename = \"short_story.txt\"\n",
    "if os.path.exists(output_filename):\n",
    "    os.remove(output_filename)\n",
    "\n",
    "short_story = \"\"\n",
    "for outline in chapter_outlines:\n",
    "    #print(outline)\n",
    "    chapter = write_chapter(outline, logline, chapter_outlines)\n",
    "    short_story += chapter\n",
    "    save_to_file(output_filename, chapter)\n",
    "\n",
    "print(short_story)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be26ac2-3495-45e2-84c7-6e0493bcf566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is a WIP of what could be a great GPT-4 version. More complex, maybe overly complex, but a good base.\n",
    "#To be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb115066-7948-460c-bb58-d616b635e388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "\n",
    "model = \"gpt-4\"\n",
    "\n",
    "def generate_story_beats(logline):\n",
    "    print(\"Generating Story Beats\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant with expertise in screenwriting and Truby's story structure. You are working on a short story so make sure the pace is fast.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Outline a short story based on this logline using Truby's story structure: {logline}. Include a big reveal or twist that will leave the audience speechless.\"}\n",
    "      ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=2048)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def refine_story_beats(story_beats):\n",
    "    print(\"Refining Story Beats\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can refine story beats to the level of a New York Times bestseller.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"You are working on a short story. Refine these story beats: {story_beats}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=2048)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def generate_chapter_outlines(story_beats):\n",
    "    print(\"Generating Chapters Outline\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Based on this short story outline: {story_beats}, generate short chapter outlines for the short story. Answer in JSON format where chapters are in a list inside the chapters attribute. It should look like this: {{\n",
    "   \"title\": \"the title\",\n",
    "   \"author\": \"the author\",\n",
    "   \"genre\": \"the genre\",\n",
    "   \"chapters\": [\n",
    "      {{\n",
    "         \"chapter_number\": 1,\n",
    "         \"chapter_title\": \"The title\",\n",
    "         \"chapter_summary\": \"the summary\"\n",
    "      }}\n",
    "   ]\n",
    "}}\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=2048)\n",
    "    chapter_outlines_str = response['choices'][0]['message']['content']\n",
    "    novel_structure = json.loads(chapter_outlines_str)\n",
    "    return novel_structure\n",
    "\n",
    "def refine_chapter_outlines(chapter_outlines):\n",
    "    print(\"Refining Chapters Outline\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can refine chapter outlines to the level of a New York Times bestseller.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Refine these chapter outlines in JSON format using double quotes: {chapter_outlines}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=2048)\n",
    "    \n",
    "    chapter_outlines_str = response['choices'][0]['message']['content']\n",
    "    #print(chapter_outlines_str)\n",
    "    novel_structure = json.loads(chapter_outlines_str)\n",
    "    return novel_structure\n",
    "\n",
    "def generate_context(outline, story_elements):\n",
    "    print(\"Generating Context for the chapter\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can generate context for a short story.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate context based on this chapter outline: {outline} and previous story elements: {story_elements}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=2048)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def write_chapter(outline, chapter_outlines, context, segments, logline):\n",
    "    print(\"Writing the first draft of the chapter\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant writing the script for a short story. When the chapter is finished, write END CHAPTER.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a chapter for a short story based on this outline: {outline}. The logline of the story is this: {logline}. This is the full list of the chapters, but remember you are only writing one of them. {chapter_outlines}. Here is the context of the story {context} When the chapter is done write END CHAPTER\"}\n",
    "    ]\n",
    "    \n",
    "    for segment in segments[-5:]:\n",
    "        messages.append({\"role\":\"assistant\", \"content\":segment})\n",
    "        messages.append({\"role\":\"user\", \"content\":\"continue this chapter. When the chapter is over write END CHAPTER\"})\n",
    "    #print(messages)\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=4000, stop=[\"END CHAPTER\"])\n",
    "    return response['choices'][0]['message']['content']\n",
    "def refine_chapter_segment(segment):\n",
    "    print(\"Refining the draft of the chapter\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can rewrite text in the style of a New York Time Best Seller.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Rewrite this text: {segment}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=4000)\n",
    "    print(response)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def evaluate_and_review_chapter(chapter):\n",
    "    print(\"Evaluating the quality of the story\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant that can evaluate and review written work.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Evaluate and review this chapter: {chapter}\"}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, max_tokens=8000)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "logline = \"A teenager gets to meet her older self from the future\"\n",
    "\n",
    "story_beats = generate_story_beats(logline)\n",
    "refined_story_beats = refine_story_beats(story_beats)\n",
    "chapter_outlines = generate_chapter_outlines(refined_story_beats)\n",
    "refined_chapter_outlines = refine_chapter_outlines(chapter_outlines)\n",
    "\n",
    "outline_filename = \"chapters.txt\"\n",
    "output_filename = \"short_story.txt\"\n",
    "review_filename = \"review.txt\"\n",
    "\n",
    "if os.path.exists(outline_filename):\n",
    "    os.remove(outline_filename)\n",
    "if os.path.exists(output_filename):\n",
    "    os.remove(output_filename)\n",
    "if os.path.exists(review_filename):\n",
    "    os.remove(review_filename)\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "save_to_file(outline_filename, json.dumps(refined_chapter_outlines))\n",
    "short_story = \"\"\n",
    "for index, outline in enumerate(refined_chapter_outlines[\"chapters\"]):\n",
    "    print(f\"Working on Chapter {index}\")\n",
    "    chapter = f\"CHAPTER {refined_chapter_outlines['chapters'][index]['chapter_number']}: {refined_chapter_outlines['chapters'][index]['chapter_title']}\"\n",
    "    context = generate_context(outline, refined_chapter_outlines[\"chapters\"][:index])\n",
    "    segments = []\n",
    "    while \"END CHAPTER\" not in chapter:\n",
    "        segment = write_chapter(outline, refined_chapter_outlines, context, segments, logline)\n",
    "        #print(segment)\n",
    "        #segment = chapter.split()[-50:]\n",
    "        refined_segment = refine_chapter_segment(segment)\n",
    "        segments.append(segment)\n",
    "        chapter += refined_segment\n",
    "        if \"END CHAPTER\" in segment:\n",
    "            #print(\"End of Chapter\")\n",
    "            #print(chapter)\n",
    "            break\n",
    "\n",
    "    chapter = chapter.replace(\"END CHAPTER\", \"\")\n",
    "    print(chapter)\n",
    "    short_story += chapter\n",
    "    save_to_file(output_filename, chapter)\n",
    "\n",
    "    review = evaluate_and_review_chapter(chapter)\n",
    "    save_to_file(review_filename, review)\n",
    "\n",
    "print(short_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f7260-5070-4194-9ec6-bb5968260f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
